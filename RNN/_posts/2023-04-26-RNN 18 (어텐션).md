---
tag: [Deep Learning, 딥러닝, from scratch, 밑바닥부터시작하는딥러닝, RNN, 자연어처리, 순환신경망, LSTM, Sequence2Seqence, Seq2Seq, 시퀀스투시퀀스, 문장생성, Attention, 어텐션, Date Dataset]
toc: true
toc_sticky: true
toc_label: 목차
author_profile: false

---

# 어텐션

## 1. 어텐션의 구조

### 1.1 seq2seq의 문제점


```python
from IPython.display import Image
Image('./deep_learning_2_images/fig 8-1.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_3_0.png)
    



### 1.2 Encoder 개선


```python
Image('./deep_learning_2_images/fig 8-2.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_5_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-3.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_6_0.png)
    



### 1.3 Decoder 개선 (1)


```python
Image('./deep_learning_2_images/fig 8-4.png', width=300)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_8_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-5.png', width=700)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_9_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-6.png', width=600)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_10_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-7.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_11_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-8.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_12_0.png)
    




```python
import numpy as np

T, H = 5, 4
hs = np.random.randn(T, H)
a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])

ar = a.reshape(5, 1).repeat(4, axis=1)
print(ar.shape)

t = hs * ar
print(t.shape)

c = np.sum(t, axis=0)
print(c.shape)
```

    (5, 4)
    (5, 4)
    (4,)
    


```python
Image('./deep_learning_2_images/fig 8-9.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_14_0.png)
    



**미니배치 처리용 가중합 구현**


```python
N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
a = np.random.randn(N, T)

ar = a.reshape(N, T, 1).repeat(H, axis=2)

t = hs * ar
print(t.shape)

c = np.sum(t, axis=1)
print(c.shape)
```

    (10, 5, 4)
    (10, 4)
    


```python
Image('./deep_learning_2_images/fig 8-11.png', width=250)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_17_0.png)
    



**Weight Sum 계층**


```python
class WeightSum:
    def __init__(self):
        self.params, self.grads = [], []
        self.cache = None

    def forward(self, hs, a):
        N, T, H = hs.shape        
        ar = a.reshape(N, T, 1).repeat(H, axis=2)   
        # ar = a.reshape(N, T, 1) # 브로드캐스트를 사용하는 경우
        t = hs * ar
        c = np.sum(t, axis=1)

        self.cache = (hs, ar)
        return c

    def backward(self, dc):
        hs, ar = self.cache
        N, T, H = hs.shape
        dt = dc.reshape(N, 1, H).repeat(T, axis=1)
        dar = dt * hs
        dhs = dt * ar
        da = np.sum(dar, axis=2)

        return dhs, da
```

### 1.4 Decoder 개선 (2)


```python
Image('./deep_learning_2_images/fig 8-12.png', width=400)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_21_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-13.png', width=400)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_22_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-14.png', width=300)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_23_0.png)
    




```python
from common.layers import Softmax

N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
h = np.random.randn(N, H)
hr = h.reshape(N, 1, H).repeat(T, axis=1)

t = hs * hr
print(t.shape)

s = np.sum(t, axis=2)
print(s.shape)

softmax = Softmax()
a = softmax.forward(s)
print(a.shape)
```

    (10, 5, 4)
    (10, 5)
    (10, 5)
    


```python
Image('./deep_learning_2_images/fig 8-15.png', width=300)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_25_0.png)
    




```python
class AttentionWeight:
    def __init__(self):
        self.params, self.grads = [], []
        self.softmax = Softmax()
        self.cache = None

    def forward(self, hs, h):
        N, T, H = hs.shape

        hr = h.reshape(N, 1, H).repeat(T, axis=1)
        # hr = h.reshape(N, 1, H) # 브로드캐스트를 사용하는 경우
        t = hs * hr
        s = np.sum(t, axis=2)
        a = self.softmax.forward(s)

        self.cache = (hs, hr)
        return a

    def backward(self, da):
        hs, hr = self.cache
        N, T, H = hs.shape

        ds = self.softmax.backward(da)
        dt = ds.reshape(N, T, 1).repeat(H, axis=2)
        dhs = dt * hr
        dhr = dt * hs
        dh = np.sum(dhr, axis=1)

        return dhs, dh
```

### 1.5 Decoder 개선 (3)


```python
Image('./deep_learning_2_images/fig 8-17.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_28_0.png)
    




```python
class Attention:
    def __init__(self):
        self.params, self.grads = [], []
        self.attention_weight_layer = AttentionWeight()
        self.weight_sum_layer = WeightSum()
        self.attention_weight = None

    def forward(self, hs, h):
        a = self.attention_weight_layer.forward(hs, h)
        out = self.weight_sum_layer.forward(hs, a)
        self.attention_weight = a
        return out

    def backward(self, dout):
        dhs0, da = self.weight_sum_layer.backward(dout)
        dhs1, dh = self.attention_weight_layer.backward(da)
        dhs = dhs0 + dhs1
        return dhs, dh
```


```python
Image('./deep_learning_2_images/fig 8-18.png', width=600)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_30_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-19.png', width=450)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_31_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-20.png', width=600)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_32_0.png)
    




```python
class TimeAttention:
    def __init__(self):
        self.params, self.grads = [], []
        self.layers = None
        self.attention_weights = None

    def forward(self, hs_enc, hs_dec):
        N, T, H = hs_dec.shape
        out = np.empty_like(hs_dec)
        self.layers = []
        self.attention_weights = []

        for t in range(T):
            layer = Attention()
            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])
            self.layers.append(layer)
            self.attention_weights.append(layer.attention_weight)

        return out

    def backward(self, dout):
        N, T, H = dout.shape
        dhs_enc = 0
        dhs_dec = np.empty_like(dout)

        for t in range(T):
            layer = self.layers[t]
            dhs, dh = layer.backward(dout[:, t, :])
            dhs_enc += dhs
            dhs_dec[:,t,:] = dh

        return dhs_enc, dhs_dec
```

## 2. 어텐션을 갖춘 seq2seq 구현

### 2.1 Encoder 구현

**AttentionEncoder 클래스**


```python
class Encoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)

        self.params = self.embed.params + self.lstm.params
        self.grads = self.embed.grads + self.lstm.grads
        self.hs = None

    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        self.hs = hs
        return hs[:, -1, :]

    def backward(self, dh):
        dhs = np.zeros_like(self.hs)
        dhs[:, -1, :] = dh

        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout
```


```python
class AttentionEncoder(Encoder):
    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        return hs

    def backward(self, dhs):
        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout
```

### 2.2 Decoder 구현


```python
Image('./deep_learning_2_images/fig 8-21.png', width=500)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_40_0.png)
    




```python
class AttentionDecoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        self.attention = TimeAttention()
        self.affine = TimeAffine(affine_W, affine_b)
        layers = [self.embed, self.lstm, self.attention, self.affine]

        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, xs, enc_hs):
        h = enc_hs[:,-1]
        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        dec_hs = self.lstm.forward(out)
        c = self.attention.forward(enc_hs, dec_hs)
        out = np.concatenate((c, dec_hs), axis=2)
        score = self.affine.forward(out)

        return score

    def backward(self, dscore):
        dout = self.affine.backward(dscore)
        N, T, H2 = dout.shape
        H = H2 // 2

        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]
        denc_hs, ddec_hs1 = self.attention.backward(dc)
        ddec_hs = ddec_hs0 + ddec_hs1
        dout = self.lstm.backward(ddec_hs)
        dh = self.lstm.dh
        denc_hs[:, -1] += dh
        self.embed.backward(dout)

        return denc_hs

    def generate(self, enc_hs, start_id, sample_size):
        sampled = []
        sample_id = start_id
        h = enc_hs[:, -1]
        self.lstm.set_state(h)

        for _ in range(sample_size):
            x = np.array([sample_id]).reshape((1, 1))

            out = self.embed.forward(x)
            dec_hs = self.lstm.forward(out)
            c = self.attention.forward(enc_hs, dec_hs)
            out = np.concatenate((c, dec_hs), axis=2)
            score = self.affine.forward(out)

            sample_id = np.argmax(score.flatten())
            sampled.append(sample_id)

        return sampled
```

### 2.3 Seq2Seq 구현


```python
from common.base_model import BaseModel
class Seq2seq(BaseModel):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        self.encoder = Encoder(V, D, H)
        self.decoder = Decoder(V, D, H)
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads

    def forward(self, xs, ts):
        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]

        h = self.encoder.forward(xs)
        score = self.decoder.forward(decoder_xs, h)
        loss = self.softmax.forward(score, decoder_ts)
        return loss

    def backward(self, dout=1):
        dout = self.softmax.backward(dout)
        dh = self.decoder.backward(dout)
        dout = self.encoder.backward(dh)
        return dout

    def generate(self, xs, start_id, sample_size):
        h = self.encoder.forward(xs)
        sampled = self.decoder.generate(h, start_id, sample_size)
        return sampled
```


```python
class AttentionSeq2seq(Seq2seq):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        args = vocab_size, wordvec_size, hidden_size
        self.encoder = AttentionEncoder(*args)
        self.decoder = AttentionDecoder(*args)
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads
```

## 3. 어텐션 평가

### 3.1 날짜 형식 변환 문제


```python
Image('./deep_learning_2_images/fig 8-22.png', width=300)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_47_0.png)
    




```python
Image('./deep_learning_2_images/fig 8-23.png', width=400)
```




    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_48_0.png)
    



### 3.2 어텐션을 갖춘 seq2seq의 학습

**날짜 변환용 데이터셋으로 AttentionSeq2seq 학습**


```python
import sys
import numpy as np
import matplotlib.pyplot as plt
from dataset import sequence
from common.optimizer import Adam
from common.trainer import Trainer
from common.util import eval_seq2seq
from common.time_layers import *

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')
char_to_id, id_to_char = sequence.get_vocab()

# 입력 문장 반전
x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]

# 하이퍼파라미터 설정
vocab_size = len(char_to_id)
wordvec_size = 16
hidden_size = 256
batch_size = 128
max_epoch = 10
max_grad = 5.0

model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)

optimizer = Adam()
trainer = Trainer(model, optimizer)

acc_list = []
for epoch in range(max_epoch):
    trainer.fit(x_train, t_train, max_epoch=1,
                batch_size=batch_size, max_grad=max_grad)

    correct_num = 0
    for i in range(len(x_test)):
        question, correct = x_test[[i]], t_test[[i]]
        verbose = i < 10
        correct_num += eval_seq2seq(model, question, correct,
                                    id_to_char, verbose, is_reverse=True)

    acc = float(correct_num) / len(x_test)
    acc_list.append(acc)
    print('정확도 %.3f%%' % (acc * 100))

model.save_params()
```

    | 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08
    | 에폭 1 |  반복 21 / 351 | 시간 18[s] | 손실 3.09
    | 에폭 1 |  반복 41 / 351 | 시간 33[s] | 손실 1.90
    | 에폭 1 |  반복 61 / 351 | 시간 48[s] | 손실 1.72
    | 에폭 1 |  반복 81 / 351 | 시간 64[s] | 손실 1.46
    | 에폭 1 |  반복 101 / 351 | 시간 78[s] | 손실 1.19
    | 에폭 1 |  반복 121 / 351 | 시간 93[s] | 손실 1.14
    | 에폭 1 |  반복 141 / 351 | 시간 109[s] | 손실 1.09
    | 에폭 1 |  반복 161 / 351 | 시간 135[s] | 손실 1.06
    | 에폭 1 |  반복 181 / 351 | 시간 153[s] | 손실 1.04
    | 에폭 1 |  반복 201 / 351 | 시간 170[s] | 손실 1.03
    | 에폭 1 |  반복 221 / 351 | 시간 185[s] | 손실 1.02
    | 에폭 1 |  반복 241 / 351 | 시간 200[s] | 손실 1.02
    | 에폭 1 |  반복 261 / 351 | 시간 214[s] | 손실 1.01
    | 에폭 1 |  반복 281 / 351 | 시간 229[s] | 손실 1.00
    | 에폭 1 |  반복 301 / 351 | 시간 243[s] | 손실 1.00
    | 에폭 1 |  반복 321 / 351 | 시간 258[s] | 손실 1.00
    | 에폭 1 |  반복 341 / 351 | 시간 273[s] | 손실 1.00
    Q 10/15/94                     
    T 1994-10-15
    X 1978-08-11
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    X 1978-08-11
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    X 1978-08-11
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    X 1978-08-11
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    X 1978-08-11
    ---
    Q october 6, 1992              
    T 1992-10-06
    X 1978-08-11
    ---
    Q 8/23/08                      
    T 2008-08-23
    X 1978-08-11
    ---
    Q 8/30/07                      
    T 2007-08-30
    X 1978-08-11
    ---
    Q 10/28/13                     
    T 2013-10-28
    X 1978-08-11
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    X 1978-08-11
    ---
    정확도 0.000%
    | 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00
    | 에폭 2 |  반복 21 / 351 | 시간 15[s] | 손실 1.00
    | 에폭 2 |  반복 41 / 351 | 시간 30[s] | 손실 0.99
    | 에폭 2 |  반복 61 / 351 | 시간 44[s] | 손실 0.99
    | 에폭 2 |  반복 81 / 351 | 시간 60[s] | 손실 0.99
    | 에폭 2 |  반복 101 / 351 | 시간 75[s] | 손실 0.99
    | 에폭 2 |  반복 121 / 351 | 시간 91[s] | 손실 0.99
    | 에폭 2 |  반복 141 / 351 | 시간 107[s] | 손실 0.98
    | 에폭 2 |  반복 161 / 351 | 시간 122[s] | 손실 0.98
    | 에폭 2 |  반복 181 / 351 | 시간 141[s] | 손실 0.97
    | 에폭 2 |  반복 201 / 351 | 시간 157[s] | 손실 0.95
    | 에폭 2 |  반복 221 / 351 | 시간 173[s] | 손실 0.94
    | 에폭 2 |  반복 241 / 351 | 시간 190[s] | 손실 0.90
    | 에폭 2 |  반복 261 / 351 | 시간 204[s] | 손실 0.83
    | 에폭 2 |  반복 281 / 351 | 시간 221[s] | 손실 0.74
    | 에폭 2 |  반복 301 / 351 | 시간 235[s] | 손실 0.66
    | 에폭 2 |  반복 321 / 351 | 시간 250[s] | 손실 0.58
    | 에폭 2 |  반복 341 / 351 | 시간 267[s] | 손실 0.46
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    X 2006-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    X 2007-08-09
    ---
    Q 10/28/13                     
    T 2013-10-28
    X 1983-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    X 2016-11-08
    ---
    정확도 51.600%
    | 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35
    | 에폭 3 |  반복 21 / 351 | 시간 16[s] | 손실 0.30
    | 에폭 3 |  반복 41 / 351 | 시간 33[s] | 손실 0.21
    | 에폭 3 |  반복 61 / 351 | 시간 48[s] | 손실 0.14
    | 에폭 3 |  반복 81 / 351 | 시간 64[s] | 손실 0.09
    | 에폭 3 |  반복 101 / 351 | 시간 84[s] | 손실 0.07
    | 에폭 3 |  반복 121 / 351 | 시간 102[s] | 손실 0.05
    | 에폭 3 |  반복 141 / 351 | 시간 118[s] | 손실 0.04
    | 에폭 3 |  반복 161 / 351 | 시간 134[s] | 손실 0.03
    | 에폭 3 |  반복 181 / 351 | 시간 150[s] | 손실 0.03
    | 에폭 3 |  반복 201 / 351 | 시간 166[s] | 손실 0.02
    | 에폭 3 |  반복 221 / 351 | 시간 182[s] | 손실 0.02
    | 에폭 3 |  반복 241 / 351 | 시간 197[s] | 손실 0.02
    | 에폭 3 |  반복 261 / 351 | 시간 213[s] | 손실 0.01
    | 에폭 3 |  반복 281 / 351 | 시간 229[s] | 손실 0.01
    | 에폭 3 |  반복 301 / 351 | 시간 244[s] | 손실 0.01
    | 에폭 3 |  반복 321 / 351 | 시간 261[s] | 손실 0.01
    | 에폭 3 |  반복 341 / 351 | 시간 276[s] | 손실 0.01
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 99.900%
    | 에폭 4 |  반복 1 / 351 | 시간 2[s] | 손실 0.01
    | 에폭 4 |  반복 21 / 351 | 시간 18[s] | 손실 0.01
    | 에폭 4 |  반복 41 / 351 | 시간 34[s] | 손실 0.01
    | 에폭 4 |  반복 61 / 351 | 시간 52[s] | 손실 0.01
    | 에폭 4 |  반복 81 / 351 | 시간 68[s] | 손실 0.01
    | 에폭 4 |  반복 101 / 351 | 시간 84[s] | 손실 0.01
    | 에폭 4 |  반복 121 / 351 | 시간 100[s] | 손실 0.00
    | 에폭 4 |  반복 141 / 351 | 시간 116[s] | 손실 0.01
    | 에폭 4 |  반복 161 / 351 | 시간 134[s] | 손실 0.00
    | 에폭 4 |  반복 181 / 351 | 시간 150[s] | 손실 0.00
    | 에폭 4 |  반복 201 / 351 | 시간 166[s] | 손실 0.00
    | 에폭 4 |  반복 221 / 351 | 시간 182[s] | 손실 0.00
    | 에폭 4 |  반복 241 / 351 | 시간 200[s] | 손실 0.00
    | 에폭 4 |  반복 261 / 351 | 시간 216[s] | 손실 0.00
    | 에폭 4 |  반복 281 / 351 | 시간 233[s] | 손실 0.00
    | 에폭 4 |  반복 301 / 351 | 시간 249[s] | 손실 0.00
    | 에폭 4 |  반복 321 / 351 | 시간 265[s] | 손실 0.00
    | 에폭 4 |  반복 341 / 351 | 시간 280[s] | 손실 0.00
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 99.900%
    | 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00
    | 에폭 5 |  반복 21 / 351 | 시간 18[s] | 손실 0.00
    | 에폭 5 |  반복 41 / 351 | 시간 32[s] | 손실 0.00
    | 에폭 5 |  반복 61 / 351 | 시간 50[s] | 손실 0.00
    | 에폭 5 |  반복 81 / 351 | 시간 66[s] | 손실 0.00
    | 에폭 5 |  반복 101 / 351 | 시간 82[s] | 손실 0.00
    | 에폭 5 |  반복 121 / 351 | 시간 98[s] | 손실 0.00
    | 에폭 5 |  반복 141 / 351 | 시간 120[s] | 손실 0.00
    | 에폭 5 |  반복 161 / 351 | 시간 134[s] | 손실 0.00
    | 에폭 5 |  반복 181 / 351 | 시간 149[s] | 손실 0.00
    | 에폭 5 |  반복 201 / 351 | 시간 167[s] | 손실 0.00
    | 에폭 5 |  반복 221 / 351 | 시간 182[s] | 손실 0.00
    | 에폭 5 |  반복 241 / 351 | 시간 199[s] | 손실 0.00
    | 에폭 5 |  반복 261 / 351 | 시간 213[s] | 손실 0.00
    | 에폭 5 |  반복 281 / 351 | 시간 230[s] | 손실 0.00
    | 에폭 5 |  반복 301 / 351 | 시간 246[s] | 손실 0.00
    | 에폭 5 |  반복 321 / 351 | 시간 262[s] | 손실 0.00
    | 에폭 5 |  반복 341 / 351 | 시간 276[s] | 손실 0.00
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 99.920%
    | 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00
    | 에폭 6 |  반복 21 / 351 | 시간 17[s] | 손실 0.00
    | 에폭 6 |  반복 41 / 351 | 시간 39[s] | 손실 0.00
    | 에폭 6 |  반복 61 / 351 | 시간 66[s] | 손실 0.00
    | 에폭 6 |  반복 81 / 351 | 시간 85[s] | 손실 0.00
    | 에폭 6 |  반복 101 / 351 | 시간 137[s] | 손실 0.00
    | 에폭 6 |  반복 121 / 351 | 시간 177[s] | 손실 0.00
    | 에폭 6 |  반복 141 / 351 | 시간 199[s] | 손실 0.00
    | 에폭 6 |  반복 161 / 351 | 시간 216[s] | 손실 0.00
    | 에폭 6 |  반복 181 / 351 | 시간 233[s] | 손실 0.00
    | 에폭 6 |  반복 201 / 351 | 시간 250[s] | 손실 0.00
    | 에폭 6 |  반복 221 / 351 | 시간 268[s] | 손실 0.00
    | 에폭 6 |  반복 241 / 351 | 시간 286[s] | 손실 0.00
    | 에폭 6 |  반복 261 / 351 | 시간 302[s] | 손실 0.00
    | 에폭 6 |  반복 281 / 351 | 시간 322[s] | 손실 0.00
    | 에폭 6 |  반복 301 / 351 | 시간 348[s] | 손실 0.00
    | 에폭 6 |  반복 321 / 351 | 시간 364[s] | 손실 0.00
    | 에폭 6 |  반복 341 / 351 | 시간 382[s] | 손실 0.00
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 99.920%
    | 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00
    | 에폭 7 |  반복 21 / 351 | 시간 16[s] | 손실 0.00
    | 에폭 7 |  반복 41 / 351 | 시간 34[s] | 손실 0.00
    | 에폭 7 |  반복 61 / 351 | 시간 50[s] | 손실 0.00
    | 에폭 7 |  반복 81 / 351 | 시간 65[s] | 손실 0.00
    | 에폭 7 |  반복 101 / 351 | 시간 83[s] | 손실 0.00
    | 에폭 7 |  반복 121 / 351 | 시간 99[s] | 손실 0.00
    | 에폭 7 |  반복 141 / 351 | 시간 114[s] | 손실 0.00
    | 에폭 7 |  반복 161 / 351 | 시간 133[s] | 손실 0.00
    | 에폭 7 |  반복 181 / 351 | 시간 149[s] | 손실 0.00
    | 에폭 7 |  반복 201 / 351 | 시간 172[s] | 손실 0.00
    | 에폭 7 |  반복 221 / 351 | 시간 191[s] | 손실 0.00
    | 에폭 7 |  반복 241 / 351 | 시간 206[s] | 손실 0.00
    | 에폭 7 |  반복 261 / 351 | 시간 228[s] | 손실 0.00
    | 에폭 7 |  반복 281 / 351 | 시간 248[s] | 손실 0.00
    | 에폭 7 |  반복 301 / 351 | 시간 266[s] | 손실 0.00
    | 에폭 7 |  반복 321 / 351 | 시간 281[s] | 손실 0.00
    | 에폭 7 |  반복 341 / 351 | 시간 297[s] | 손실 0.00
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 99.920%
    | 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00
    | 에폭 8 |  반복 21 / 351 | 시간 19[s] | 손실 0.00
    | 에폭 8 |  반복 41 / 351 | 시간 33[s] | 손실 0.00
    | 에폭 8 |  반복 61 / 351 | 시간 52[s] | 손실 0.00
    | 에폭 8 |  반복 81 / 351 | 시간 73[s] | 손실 0.00
    | 에폭 8 |  반복 101 / 351 | 시간 96[s] | 손실 0.00
    | 에폭 8 |  반복 121 / 351 | 시간 116[s] | 손실 0.00
    | 에폭 8 |  반복 141 / 351 | 시간 136[s] | 손실 0.00
    | 에폭 8 |  반복 161 / 351 | 시간 155[s] | 손실 0.00
    | 에폭 8 |  반복 181 / 351 | 시간 178[s] | 손실 0.00
    | 에폭 8 |  반복 201 / 351 | 시간 197[s] | 손실 0.00
    | 에폭 8 |  반복 221 / 351 | 시간 215[s] | 손실 0.00
    | 에폭 8 |  반복 241 / 351 | 시간 236[s] | 손실 0.00
    | 에폭 8 |  반복 261 / 351 | 시간 256[s] | 손실 0.00
    | 에폭 8 |  반복 281 / 351 | 시간 273[s] | 손실 0.00
    | 에폭 8 |  반복 301 / 351 | 시간 289[s] | 손실 0.00
    | 에폭 8 |  반복 321 / 351 | 시간 308[s] | 손실 0.00
    | 에폭 8 |  반복 341 / 351 | 시간 323[s] | 손실 0.00
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 99.960%
    | 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00
    | 에폭 9 |  반복 21 / 351 | 시간 19[s] | 손실 0.00
    | 에폭 9 |  반복 41 / 351 | 시간 34[s] | 손실 0.00
    | 에폭 9 |  반복 61 / 351 | 시간 51[s] | 손실 0.00
    | 에폭 9 |  반복 81 / 351 | 시간 65[s] | 손실 0.00
    | 에폭 9 |  반복 101 / 351 | 시간 82[s] | 손실 0.00
    | 에폭 9 |  반복 121 / 351 | 시간 98[s] | 손실 0.00
    | 에폭 9 |  반복 141 / 351 | 시간 114[s] | 손실 0.00
    | 에폭 9 |  반복 161 / 351 | 시간 131[s] | 손실 0.00
    | 에폭 9 |  반복 181 / 351 | 시간 148[s] | 손실 0.00
    | 에폭 9 |  반복 201 / 351 | 시간 166[s] | 손실 0.00
    | 에폭 9 |  반복 221 / 351 | 시간 186[s] | 손실 0.00
    | 에폭 9 |  반복 241 / 351 | 시간 207[s] | 손실 0.00
    | 에폭 9 |  반복 261 / 351 | 시간 232[s] | 손실 0.00
    | 에폭 9 |  반복 281 / 351 | 시간 255[s] | 손실 0.00
    | 에폭 9 |  반복 301 / 351 | 시간 272[s] | 손실 0.00
    | 에폭 9 |  반복 321 / 351 | 시간 289[s] | 손실 0.00
    | 에폭 9 |  반복 341 / 351 | 시간 305[s] | 손실 0.00
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 99.960%
    | 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00
    | 에폭 10 |  반복 21 / 351 | 시간 20[s] | 손실 0.00
    | 에폭 10 |  반복 41 / 351 | 시간 38[s] | 손실 0.00
    | 에폭 10 |  반복 61 / 351 | 시간 57[s] | 손실 0.00
    | 에폭 10 |  반복 81 / 351 | 시간 73[s] | 손실 0.00
    | 에폭 10 |  반복 101 / 351 | 시간 91[s] | 손실 0.00
    | 에폭 10 |  반복 121 / 351 | 시간 107[s] | 손실 0.00
    | 에폭 10 |  반복 141 / 351 | 시간 125[s] | 손실 0.00
    | 에폭 10 |  반복 161 / 351 | 시간 144[s] | 손실 0.00
    | 에폭 10 |  반복 181 / 351 | 시간 166[s] | 손실 0.00
    | 에폭 10 |  반복 201 / 351 | 시간 184[s] | 손실 0.09
    | 에폭 10 |  반복 221 / 351 | 시간 203[s] | 손실 0.01
    | 에폭 10 |  반복 241 / 351 | 시간 222[s] | 손실 0.01
    | 에폭 10 |  반복 261 / 351 | 시간 243[s] | 손실 0.00
    | 에폭 10 |  반복 281 / 351 | 시간 260[s] | 손실 0.00
    | 에폭 10 |  반복 301 / 351 | 시간 276[s] | 손실 0.00
    | 에폭 10 |  반복 321 / 351 | 시간 293[s] | 손실 0.00
    | 에폭 10 |  반복 341 / 351 | 시간 310[s] | 손실 0.00
    Q 10/15/94                     
    T 1994-10-15
    O 1994-10-15
    ---
    Q thursday, november 13, 2008  
    T 2008-11-13
    O 2008-11-13
    ---
    Q Mar 25, 2003                 
    T 2003-03-25
    O 2003-03-25
    ---
    Q Tuesday, November 22, 2016   
    T 2016-11-22
    O 2016-11-22
    ---
    Q Saturday, July 18, 1970      
    T 1970-07-18
    O 1970-07-18
    ---
    Q october 6, 1992              
    T 1992-10-06
    O 1992-10-06
    ---
    Q 8/23/08                      
    T 2008-08-23
    O 2008-08-23
    ---
    Q 8/30/07                      
    T 2007-08-30
    O 2007-08-30
    ---
    Q 10/28/13                     
    T 2013-10-28
    O 2013-10-28
    ---
    Q sunday, november 6, 2016     
    T 2016-11-06
    O 2016-11-06
    ---
    정확도 100.000%
    


```python
plt.rc("font", family="Malgun Gothic")
# 그래프 그리기
x = np.arange(len(acc_list))
plt.plot(x, acc_list, marker='o')
plt.xlabel('에폭')
plt.ylabel('정확도')
plt.ylim(-0.05, 1.05)
plt.show()
```


    
![png](/assets/images/2023-04-26-RNN 18 (어텐션)/output_52_0.png)
    



```python

```
